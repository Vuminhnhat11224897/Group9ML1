{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MyPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "np.random.seed(40)\n",
    "\n",
    "# Define the range for x1, x2, x3\n",
    "x_range = (0, 10)\n",
    "\n",
    "# Define the sequence of θ values\n",
    "theta_values = [7, 8, 12, 13, 14, 9, 8, 7, 13, 12, 13, 11, 15, 16, 14]\n",
    "\n",
    "# Total number of samples\n",
    "total_samples = 75000\n",
    "\n",
    "# Number of samples per θ value\n",
    "samples_per_theta = total_samples // len(theta_values)\n",
    "\n",
    "# Generate the dataset using vectorized operations\n",
    "x1 = np.random.uniform(*x_range, total_samples)\n",
    "x2 = np.random.uniform(*x_range, total_samples)\n",
    "x3 = np.random.uniform(*x_range, total_samples)\n",
    "theta_repeated = np.repeat(theta_values, samples_per_theta)\n",
    "labels = np.where(x1 + x2 > theta_repeated, '1', '0')\n",
    "\n",
    "# Create a NumPy array\n",
    "data = np.column_stack((x1, x2, x3, labels))\n",
    "\n",
    "def create_datastream(data, batch_size):\n",
    "    return [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "def train_random_forest_classifier(X, y, n_estimators=50):\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=-1)\n",
    "    rf.fit(X, y)\n",
    "    return rf\n",
    "\n",
    "def adaptive_ensemble_size(C, sample, unique_label, anpha, min_num=3):\n",
    "    probability_list = []\n",
    "    for label in unique_label:\n",
    "        li = []\n",
    "        for clf in C:\n",
    "            probabilities = clf.predict_proba(sample)[0]\n",
    "            current_proba = probabilities[clf.classes_ == label][0]\n",
    "            if len(li) < min_num or abs(current_proba - li[-1]) < anpha:\n",
    "                li.append(current_proba)\n",
    "            else:\n",
    "                break\n",
    "        probability_list.append(li)\n",
    "    return probability_list\n",
    "\n",
    "def linear_regression(x, y):\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    return np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "\n",
    "def tendency_prediction(probability_list, Y, epsilon=0.01):\n",
    "    predicted_probabilities = []\n",
    "    for li in probability_list:\n",
    "        x = np.arange(1, len(li) + 1)\n",
    "        y = np.array(li)\n",
    "        slope, intercept = linear_regression(x, y)\n",
    "        next_value = slope * (len(li) + 2) + intercept\n",
    "        li.insert(0, next_value)\n",
    "        weighted_prob = sum([li[x - 1] * (1 - x * epsilon) for x in range(1, len(li) + 1)]) / len(li)\n",
    "        predicted_probabilities.append(weighted_prob)\n",
    "    return Y[np.argmax(predicted_probabilities)]\n",
    "\n",
    "def predict_sample(C, row, unique_labels, len_Bi_1):\n",
    "    sample = row[:-1].reshape(1, -1).astype(float)\n",
    "    anpha = (1500 / len_Bi_1) * 0.2\n",
    "    selected_classifiers = adaptive_ensemble_size(C, sample, unique_labels, anpha)\n",
    "    return tendency_prediction(selected_classifiers, unique_labels)\n",
    "\n",
    "def process_data_stream(S, m, k, unique_labels):\n",
    "    C = deque(maxlen=m)\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    block_accuracies = []\n",
    "    prediction_times = []\n",
    "\n",
    "    for i in range(len(S) - 1):\n",
    "        Bi = S[i]\n",
    "        X = Bi[:, :-1].astype(float)\n",
    "        y = Bi[:, -1]\n",
    "        Ci = train_random_forest_classifier(X, y)\n",
    "        C.append(Ci)\n",
    "        if len(C) < k:\n",
    "            continue\n",
    "        Bi_1 = S[i + 1]\n",
    "        start_time = time.time()\n",
    "        block_predictions = Parallel(n_jobs=-1)(delayed(predict_sample)(C, row, unique_labels, len(Bi_1)) for row in Bi_1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        prediction_times.append(end_time - start_time)\n",
    "        \n",
    "        true_labels.extend(Bi_1[:, -1])\n",
    "        predicted_labels.extend(block_predictions)\n",
    "        block_accuracies.append(accuracy_score(true_labels[-len(Bi_1):], predicted_labels[-len(Bi_1):]))\n",
    "\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(block_accuracies, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Accuracy of Each Block')\n",
    "    plt.xlabel('Block Index')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(prediction_times, marker='o', linestyle='-', color='r')\n",
    "    plt.title('Prediction Time of Each Block')\n",
    "    plt.xlabel('Block Index')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "batch_size = 500\n",
    "S = create_datastream(data, batch_size)\n",
    "m = 15\n",
    "k = 3\n",
    "unique_labels = np.unique(data[:, -1])\n",
    "process_data_stream(S, m, k, unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
